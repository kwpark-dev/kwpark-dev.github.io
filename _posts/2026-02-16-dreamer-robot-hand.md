---
title: "Dreaming Robot Hand Manipulation"
date: 2026-02-16
tags: [RL, Vision]
---

{% include mathjax.html %}

<p>{{ page.tags | join: ", #" | prepend: "#" }}</p>

## Introduction

### Overview
Dreamer is a model-based reinforcement learning framework that trains an agent using imagined trajectories generated by a learned world model, rather than relying solely on direct interaction with the real environment. By learning a latent dynamical model of the environment, Dreamer enables policy learning through imagination—analogous to mental imagery, where athletes improve performance by internally simulating movements rather than executing them physically.

The Dreamer framework consists of two major components: a world model and an agent (policy and value learning framework). The world model predicts latent dynamics, observations, rewards, and continuation (termination) signals, while the agent is trained entirely on trajectories imagined within this latent space. The world model in Dreamer combines ideas from the earlier World Models (WM) framework proposed by Ha and Schmidhuber with the Recurrent State Space Model (RSSM). RSSM integrates deterministic recurrent states with stochastic latent variables, allowing the model to capture both temporal structure and uncertainty under partial observability.

Although Dreamer proposes a specific actor–critic learning procedure optimized for training in latent space, the agent component is not inherently tied to a particular reinforcement learning algorithm. In principle, alternative control or planning methods may be employed, provided they are compatible with the intrinsic dynamics induced by the learned latent world model.

### Theoretical Idea
Dreamer separates training loop for the world model and the controller but, the both shares latent states $s_t$ factorized as $h_t, z_t$. $h_t$ is recurrent memory defined as $h_t = f_\theta (h_{t-1}, z_{t-1}, a_{t_1})$, whereas $z_t$ is a sample of $q(z_t|h_t, o_t)$, a stochastic variable. Following illustration describes analogy behind the state factorization. Keep in mind, $z_t$ is something else sampled from the trained distribution, not extracted features from the encoder. The variable $o_t$ corresponds to image features.

<p align="center">
  <img src="/assets/images/dreamer/illustration.png" width="500">
  <br>
  <em> Sequential model predicts deterministic state and stochastic state corrects the prediction. The red dashed line implies the ground truth.   </em>
</p>

Note that the world model in Dreamer is naturally multi head architecture that prdicts reward, continuation (done mask), and latent state prediction at once, which corresponds to extended version of WM. In this project, [slot attetion](https://proceedings.neurips.cc/paper/2020/file/8511df98c02ab60aea1b2356c013bc0f-Paper.pdf), that is the additional means to improve recognition capability are considered. It is unsupervised deep neural network for object discovery via attention mechanism, which decomposes a scene into object-centric frames (in ideal case). Thus, latent state would contain features for entities (pixel clusters) rather than raw pixel information. The task space tackled by Dreamer consists of a robot hand and a block. If the scenes are decomposed *ideally*, each slot would contain different objects, the robot hand and the block(s). For the better texture quality, L1 loss is added.

<p align="center">
  <img src="/assets/images/dreamer/recon_compare.png" width="500">
  <br>
  <em> The left panel is trained by MSE loss without significant modification. The right panel clearly shows that L1 loss enhances sharpness of the reconstructed.  </em>
</p>

However, slot attention struggles to identify the objects. It would be challenging to decompose the complex scenes into independent objects. Like "a hand grasping a cup", unsupervised learning lacks clues to detach them. Nevertheless, it might help RL agent's scene understanding since slot attention groups pixel information to trigger inductive bias but, it isn't well-aligned with Human's knowledge.

<p align="center">
  <img src="/assets/images/dreamer/mask_compare.png" width="900">
  <br>
  <em> Both up & down panels fail to identify the objects but machine can build its own inductive bias. </em>
</p>


## Configurations
Dreamer mainly comprises world model including dynamic transition, reward and termination mask as well as behavioral model based on actor-critic approach. PhyXDreamer considers pixel clusters rather than raw individual pixels to enhance physical interpretability by triggering inductive bias. 

<p align="center">
  <img src="./figures/configuration.png" width="500">
  <br>
  <em> Configuration of PhyXDreamer. Slot attention is added to vision model so that the agents can understand the scene in entity-wise manner.  </em>
</p>

### Environment



### Model


### Framework


## Results


## Reference

1. [DreamerV1](https://arxiv.org/pdf/1912.01603)
2. [Slot Attention](https://proceedings.neurips.cc/paper/2020/file/8511df98c02ab60aea1b2356c013bc0f-Paper.pdf)
3. [World Model](https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)